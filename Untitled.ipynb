{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1bca205a",
   "metadata": {},
   "source": [
    "# Importing all the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "385b5ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "# *\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.metrics import BinaryAccuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a49ef2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Dataset in a variable\n",
    "df = pd.read_csv('Problem_Dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e22b404b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will save Obs and Type column respectively\n",
    "X = df['Obs'].values\n",
    "y = df['Type'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "26497cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "# Converting Strings labels to Integer\n",
    "label_encoder = LabelEncoder()\n",
    "y_encode = label_encoder.fit_transform(y)\n",
    "# calculating number of classes i.e number of labels which will be further use for encoding\n",
    "num_classes = len(label_encoder.classes_)\n",
    "print(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ba6ddc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data in train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y_encode, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ddaf154a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here tokenizer is used to convert the sentences in X_train to numeric values for every word\n",
    "tokenizer = keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_val_seq = tokenizer.texts_to_sequences(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "80c963fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here padding is done so that the variable words in sentences can be made of same size\n",
    "X_train_padded = keras.preprocessing.sequence.pad_sequences(X_train_seq)\n",
    "X_val_padded = keras.preprocessing.sequence.pad_sequences(X_val_seq, maxlen=X_train_padded.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "54b8fa94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing one hot encoding, as we are working with binary we will convert all the labels in binary and store them as one's and zero's\n",
    "y_train_oneHot = to_categorical(y_train, num_classes=num_classes)\n",
    "y_val_oneHot = to_categorical(y_val, num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e03c0d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function which will help in creating a model for every single label\n",
    "def modelCreation(activation1, activation2,labels):\n",
    "    models = {}\n",
    "    for label in labels:\n",
    "        # Define the model architecture (adjust as needed)\n",
    "        model = keras.Sequential([\n",
    "        layers.Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=100, input_length=X_train_padded.shape[1]),\n",
    "        layers.LSTM(64),\n",
    "        layers.Dense(32, activation1),\n",
    "        layers.Dense(num_classes, activation2)\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d891a4",
   "metadata": {},
   "source": [
    "# Binary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d7c70955",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelCreation(\"relu\",\"softmax\",label_encoder.classes_ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721a33c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_padded, y_train_oneHot, validation_data=(X_val_padded, y_val_oneHot), epochs=5, batch_size=32, verbose=2)\n",
    "\n",
    "models[label] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b52f00c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the models *\n",
    "for label, model in models.items():\n",
    "    y_val_pred_oneHot = model.predict(X_val_padded)\n",
    "    y_val_pred = label_encoder.inverse_transform(y_val_pred_oneHot.argmax(axis=1))\n",
    "    y_val_true = label_encoder.inverse_transform(y_val)\n",
    "\n",
    "    accuracy = accuracy_score(y_val_true, y_val_pred)\n",
    "    report = classification_report(y_val_true, y_val_pred,output_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "1d5ae4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"Label: {label}\")\n",
    "# print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
    "# print(f\"Classification Report:\\n{report}\")\n",
    "# print(\"=\"*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "08c20090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.9584755e-01 2.6452059e-01 3.6234066e-03 3.9595002e-04 1.6485045e-02\n",
      "  9.7187907e-01]\n",
      " [1.6411774e-02 9.9613396e-03 9.6898508e-01 1.9018471e-02 1.1485673e-03\n",
      "  9.9951589e-01]\n",
      " [7.4048615e-01 2.8948782e-02 8.4154733e-02 1.3777857e-03 9.9352515e-01\n",
      "  1.0086340e-02]\n",
      " ...\n",
      " [6.3254207e-02 8.3833485e-04 1.3617864e-02 9.7336346e-01 4.0961144e-04\n",
      "  9.9921101e-01]\n",
      " [9.0549594e-01 2.4006036e-03 1.2031073e-01 9.3064914e-03 9.5914114e-01\n",
      "  7.7293612e-02]\n",
      " [9.7800118e-01 1.4644356e-03 1.7673150e-02 8.4942821e-03 2.8774500e-01\n",
      "  6.4360970e-01]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report_df = pd.DataFrame(report).transpose()\n",
    "# Save the DataFrame to a CSV file\n",
    "report_df.to_csv('classification_report_Binary.csv', index=True)\n",
    "print(y_val_pred)\n",
    "print()\n",
    "# predictions_df = pd.DataFrame(y_val_pred, columns=label_encoder.classes_)\n",
    "# predictions_df.to_csv('multiLabel_predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5212b13b",
   "metadata": {},
   "source": [
    "# Multilabel Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cf992ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_binarizer = MultiLabelBinarizer()\n",
    "y_encoded = label_binarizer.fit_transform(y)\n",
    "num_classes = len(label_binarizer.classes_)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Tokenize the text data\n",
    "tokenizer = keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_val_seq = tokenizer.texts_to_sequences(X_val)\n",
    "\n",
    "# Pad sequences to ensure consistent length\n",
    "X_train_padded = keras.preprocessing.sequence.pad_sequences(X_train_seq)\n",
    "X_val_padded = keras.preprocessing.sequence.pad_sequences(X_val_seq, maxlen=X_train_padded.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "52f240d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    layers.Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=100, input_length=X_train_padded.shape[1]),\n",
    "    layers.LSTM(64),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(num_classes, activation='sigmoid')  # Sigmoid for multi-label classification\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d69f202f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "27/27 [==============================] - 3s 28ms/step - loss: 0.6622 - accuracy: 0.1381 - val_loss: 0.5949 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/10\n",
      "27/27 [==============================] - 0s 11ms/step - loss: 0.5625 - accuracy: 0.0000e+00 - val_loss: 0.5158 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/10\n",
      "27/27 [==============================] - 0s 12ms/step - loss: 0.4289 - accuracy: 0.0000e+00 - val_loss: 0.3972 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/10\n",
      "27/27 [==============================] - 0s 11ms/step - loss: 0.3489 - accuracy: 0.0000e+00 - val_loss: 0.3567 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/10\n",
      "27/27 [==============================] - 0s 12ms/step - loss: 0.2791 - accuracy: 0.0060 - val_loss: 0.2945 - val_accuracy: 0.0238\n",
      "Epoch 6/10\n",
      "27/27 [==============================] - 0s 12ms/step - loss: 0.2109 - accuracy: 0.0381 - val_loss: 0.2575 - val_accuracy: 0.0476\n",
      "Epoch 7/10\n",
      "27/27 [==============================] - 0s 12ms/step - loss: 0.1613 - accuracy: 0.0512 - val_loss: 0.2395 - val_accuracy: 0.1000\n",
      "Epoch 8/10\n",
      "27/27 [==============================] - 0s 11ms/step - loss: 0.1183 - accuracy: 0.0821 - val_loss: 0.2225 - val_accuracy: 0.1000\n",
      "Epoch 9/10\n",
      "27/27 [==============================] - 0s 13ms/step - loss: 0.0892 - accuracy: 0.1226 - val_loss: 0.2357 - val_accuracy: 0.1286\n",
      "Epoch 10/10\n",
      "27/27 [==============================] - 0s 11ms/step - loss: 0.0656 - accuracy: 0.1131 - val_loss: 0.2740 - val_accuracy: 0.1286\n",
      "7/7 [==============================] - 0s 5ms/step\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "X_val_padded = keras.preprocessing.sequence.pad_sequences(X_val_seq, maxlen=X_train_padded.shape[1])\n",
    "# Train the model with early stopping\n",
    "early_stopping = EarlyStopping(patience=3, restore_best_weights=True)\n",
    "model.fit(X_train_padded, y_train, validation_data=(X_val_padded, y_val), epochs=10, batch_size=32, callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate the model\n",
    "y_val_pred = model.predict(X_val_padded)\n",
    "y_val_pred_binary = (y_val_pred > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bb0fcebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "report = classification_report(y_val, y_val_pred_binary, target_names=label_binarizer.classes_,output_dict=True)\n",
    "# print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2c3eff9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_df = pd.DataFrame(report).transpose()\n",
    "# Save the DataFrame to a CSV file\n",
    "report_df.to_csv('classification_report_Multi.csv', index=True)\n",
    "\n",
    "predictions_df = pd.DataFrame(y_val_pred_binary, columns=label_binarizer.classes_)\n",
    "predictions_df.to_csv('multiLabel_predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a22afe3",
   "metadata": {},
   "source": [
    "# BERT Model (Some Issues) *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "b778817b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text data using BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "X_train_encoded = tokenizer(X_train.tolist(), padding=True, truncation=True, return_tensors='tf', max_length=128)\n",
    "X_val_encoded = tokenizer(X_val.tolist(), padding=True, truncation=True, return_tensors='tf', max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "fe436d1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained BERT model\n",
    "bert_model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "bd34d911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function AtomicFunction.__del__ at 0x000001FC89F15940>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ual-laptop\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py\", line 292, in __del__\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot generate a hashable key for IteratorSpec(({'input_ids': TensorSpec(shape=(None, 38), dtype=tf.int32, name=None), 'token_type_ids': TensorSpec(shape=(None, 38), dtype=tf.int32, name=None), 'attention_mask': TensorSpec(shape=(None, 38), dtype=tf.int32, name=None)}, TensorSpec(shape=(None,), dtype=tf.int32, name=None)),) because the _serialize() method returned an unsupproted value of type <class 'transformers.tokenization_utils_base.BatchEncoding'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[91], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Train the model with early stopping\u001b[39;00m\n\u001b[0;32m      7\u001b[0m early_stopping \u001b[38;5;241m=\u001b[39m EarlyStopping(patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m----> 8\u001b[0m bert_model\u001b[38;5;241m.\u001b[39mfit(X_train_encoded, y_train, validation_data\u001b[38;5;241m=\u001b[39m(X_val_encoded, y_val), epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, callbacks\u001b[38;5;241m=\u001b[39m[early_stopping])\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[0;32m     11\u001b[0m y_val_pred_logits \u001b[38;5;241m=\u001b[39m bert_model\u001b[38;5;241m.\u001b[39mpredict(X_val_encoded)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\core\\function\\polymorphism\\function_cache.py:75\u001b[0m, in \u001b[0;36mFunctionCache.add\u001b[1;34m(self, fn, context)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Adds a new function using its function_type.\u001b[39;00m\n\u001b[0;32m     69\u001b[0m \n\u001b[0;32m     70\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03m  fn: The function to be added to the cache.\u001b[39;00m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;124;03m  context: A FunctionContext representing the current context.\u001b[39;00m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     74\u001b[0m context \u001b[38;5;241m=\u001b[39m context \u001b[38;5;129;01mor\u001b[39;00m FunctionContext()\n\u001b[1;32m---> 75\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_primary[(context, fn\u001b[38;5;241m.\u001b[39mfunction_type)] \u001b[38;5;241m=\u001b[39m fn\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m context \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dispatch_dict:\n\u001b[0;32m     77\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dispatch_dict[context] \u001b[38;5;241m=\u001b[39m type_dispatch\u001b[38;5;241m.\u001b[39mTypeDispatchTable()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\core\\function\\polymorphism\\function_type.py:449\u001b[0m, in \u001b[0;36mFunctionType.__hash__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    448\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__hash__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m--> 449\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mhash\u001b[39m((\u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mitems()), \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcaptures\u001b[38;5;241m.\u001b[39mitems())))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\core\\function\\polymorphism\\function_type.py:148\u001b[0m, in \u001b[0;36mParameter.__hash__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__hash__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 148\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mhash\u001b[39m((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkind, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptional, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtype_constraint))\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot generate a hashable key for IteratorSpec(({'input_ids': TensorSpec(shape=(None, 38), dtype=tf.int32, name=None), 'token_type_ids': TensorSpec(shape=(None, 38), dtype=tf.int32, name=None), 'attention_mask': TensorSpec(shape=(None, 38), dtype=tf.int32, name=None)}, TensorSpec(shape=(None,), dtype=tf.int32, name=None)),) because the _serialize() method returned an unsupproted value of type <class 'transformers.tokenization_utils_base.BatchEncoding'>"
     ]
    }
   ],
   "source": [
    "# Compile the model\n",
    "bert_model.compile(optimizer=Adam(learning_rate=2e-5),\n",
    "                   loss=BinaryCrossentropy(),\n",
    "                   metrics=[BinaryAccuracy()])\n",
    "\n",
    "# Train the model with early stopping\n",
    "early_stopping = EarlyStopping(patience=3, restore_best_weights=True)\n",
    "bert_model.fit(X_train_encoded, y_train, validation_data=(X_val_encoded, y_val), epochs=5, batch_size=32, callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate the model\n",
    "y_val_pred_logits = bert_model.predict(X_val_encoded)\n",
    "y_val_pred = tf.sigmoid(y_val_pred_logits).numpy()\n",
    "y_val_pred_binary = (y_val_pred > 0.5).astype(int)\n",
    "\n",
    "# Report precision, recall, F1-score\n",
    "report = classification_report(y_val, y_val_pred_binary, target_names=label_binarizer.classes_)\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d133d75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
